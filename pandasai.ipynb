{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Downloading ollama...\n",
      "######################################################################## 100.0%##O#- #                                                                       \n",
      ">>> Installing ollama to /usr/local/bin...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "# Download and run the Ollama Linux install script\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "# !command -v systemctl >/dev/null && sudo systemctl stop ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.9.5)\n",
      "Requirement already satisfied: pyngrok in /usr/local/python/3.10.13/lib/python3.10/site-packages (7.1.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp) (4.0.3)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from pyngrok) (6.0.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp) (3.6)\n",
      ">>> starting ngrok config add-authtoken 26bDGGPyD3XWVtyzK3dfK401oIq_74Wi1m84A7rv9bP5YnihG\n",
      "Authtoken saved to configuration file: /home/codespace/.config/ngrok/ngrok.yml\n",
      ">>> starting ollama serve\n",
      ">>> starting ngrok http --log stderr 11434 --host-header localhost:11434\n",
      "2024/05/12 19:40:27 routes.go:1006: INFO server config env=\"map[OLLAMA_DEBUG:false OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]\"\n",
      "time=2024-05-12T19:40:27.328Z level=INFO source=images.go:704 msg=\"total blobs: 5\"\n",
      "time=2024-05-12T19:40:27.328Z level=INFO source=images.go:711 msg=\"total unused blobs removed: 0\"\n",
      "time=2024-05-12T19:40:27.329Z level=INFO source=routes.go:1052 msg=\"Listening on 127.0.0.1:11434 (version 0.1.37)\"\n",
      "time=2024-05-12T19:40:27.329Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama2705423442/runners\n",
      "t=2024-05-12T19:40:27+0000 lvl=info msg=\"no configuration paths supplied\"\n",
      "t=2024-05-12T19:40:27+0000 lvl=info msg=\"using configuration at default config path\" path=/home/codespace/.config/ngrok/ngrok.yml\n",
      "t=2024-05-12T19:40:27+0000 lvl=info msg=\"open config file\" path=/home/codespace/.config/ngrok/ngrok.yml err=nil\n",
      "t=2024-05-12T19:40:27+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
      "t=2024-05-12T19:40:28+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
      "t=2024-05-12T19:40:28+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
      "t=2024-05-12T19:40:28+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:11434 url=https://24b1-74-225-244-230.ngrok-free.app\n",
      "time=2024-05-12T19:40:31.762Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60002]\"\n",
      "time=2024-05-12T19:40:31.769Z level=INFO source=types.go:71 msg=\"inference compute\" id=0 library=cpu compute=\"\" driver=0.0 name=\"\" total=\"7.7 GiB\" available=\"175.5 MiB\"\n",
      "t=2024-05-12T19:40:35+0000 lvl=info msg=start pg=/ id=52e0edee8f331517\n",
      "t=2024-05-12T19:40:35+0000 lvl=info msg=end pg=/ id=52e0edee8f331517 status=302 dur=70.702µs\n",
      "t=2024-05-12T19:40:35+0000 lvl=info msg=start pg=/inspect/http id=8882e624e1e1cab4\n",
      "t=2024-05-12T19:40:35+0000 lvl=info msg=end pg=/inspect/http id=8882e624e1e1cab4 status=200 dur=260.957µs\n",
      "t=2024-05-12T19:40:35+0000 lvl=info msg=start pg=/static/css/app.adf18cf630a45f8cb520.css id=461f6bcc87d2157c\n",
      "t=2024-05-12T19:40:35+0000 lvl=info msg=end pg=/static/css/app.adf18cf630a45f8cb520.css id=461f6bcc87d2157c status=200 dur=4.723513ms\n",
      "t=2024-05-12T19:40:35+0000 lvl=info msg=start pg=/static/js/rpx.27bb9678091ac140b85f.js id=994f12b5e6ebcf65\n",
      "t=2024-05-12T19:40:35+0000 lvl=info msg=end pg=/static/js/rpx.27bb9678091ac140b85f.js id=994f12b5e6ebcf65 status=200 dur=128.098µs\n",
      "t=2024-05-12T19:40:35+0000 lvl=info msg=start pg=/static/js/vendor.d7e6506ba6b9826fd77a.js id=55123cc192cfa54a\n",
      "t=2024-05-12T19:40:35+0000 lvl=info msg=end pg=/static/js/vendor.d7e6506ba6b9826fd77a.js id=55123cc192cfa54a status=200 dur=1.957158ms\n",
      "t=2024-05-12T19:40:35+0000 lvl=info msg=start pg=/static/js/app.17e190cb536780ea42b0.js id=0ed27c4160dd8cac\n",
      "t=2024-05-12T19:40:35+0000 lvl=info msg=end pg=/static/js/app.17e190cb536780ea42b0.js id=0ed27c4160dd8cac status=200 dur=136.985µs\n",
      "t=2024-05-12T19:40:36+0000 lvl=info msg=start pg=/grpc/agent.Web/Preloaded id=bb90507a8b6df7d8\n",
      "t=2024-05-12T19:40:36+0000 lvl=info msg=end pg=/grpc/agent.Web/Preloaded id=bb90507a8b6df7d8 status=200 dur=1.110559ms\n",
      "t=2024-05-12T19:40:36+0000 lvl=info msg=start pg=/grpc/agent.Web/State id=64f035870b79d965\n",
      "t=2024-05-12T19:40:40+0000 lvl=info msg=start pg=/grpc/agent.Web/State id=c16e9aad69844083\n",
      "t=2024-05-12T19:40:44+0000 lvl=info msg=start pg=/grpc/agent.Web/State id=096b8d889e7a22d6\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=start pg=/inspect/http id=b13c77b6d06227ce\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=end pg=/inspect/http id=b13c77b6d06227ce status=200 dur=185.686µs\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=end pg=/grpc/agent.Web/State id=64f035870b79d965 status=200 dur=9.511633229s\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=end pg=/grpc/agent.Web/State id=c16e9aad69844083 status=200 dur=5.586498434s\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=end pg=/grpc/agent.Web/State id=096b8d889e7a22d6 status=200 dur=1.613667091s\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=start pg=/static/css/app.adf18cf630a45f8cb520.css id=8981826fac1f731e\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=end pg=/static/css/app.adf18cf630a45f8cb520.css id=8981826fac1f731e status=304 dur=149.79µs\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=start pg=/static/js/vendor.d7e6506ba6b9826fd77a.js id=13397f325e8f42fd\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=end pg=/static/js/vendor.d7e6506ba6b9826fd77a.js id=13397f325e8f42fd status=304 dur=1.069493ms\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=start pg=/static/js/rpx.27bb9678091ac140b85f.js id=42dceb908d0f5f8d\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=end pg=/static/js/rpx.27bb9678091ac140b85f.js id=42dceb908d0f5f8d status=304 dur=104.214µs\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=start pg=/static/js/app.17e190cb536780ea42b0.js id=7d2c33d5ca6f8326\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=end pg=/static/js/app.17e190cb536780ea42b0.js id=7d2c33d5ca6f8326 status=304 dur=97.682µs\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=start pg=/grpc/agent.Web/State id=e1b164500a86db8d\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=start pg=/static/favicon.ico id=8a6d41fece91e51c\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=end pg=/static/favicon.ico id=8a6d41fece91e51c status=200 dur=133.199µs\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=start pg=/grpc/agent.Web/Preloaded id=5f8c33ea95bd81e9\n",
      "t=2024-05-12T19:40:46+0000 lvl=info msg=end pg=/grpc/agent.Web/Preloaded id=5f8c33ea95bd81e9 status=200 dur=338.812µs\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m, in \u001b[0;36mrun_process.<locals>.pipe\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipe\u001b[39m(lines):\n\u001b[0;32m---> 18\u001b[0m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/asyncio/streams.py:723\u001b[0m, in \u001b[0;36mStreamReader.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__anext__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 723\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/asyncio/streams.py:524\u001b[0m, in \u001b[0;36mStreamReader.readline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreaduntil(sep)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mIncompleteReadError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/asyncio/streams.py:616\u001b[0m, in \u001b[0;36mStreamReader.readuntil\u001b[0;34m(self, separator)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;66;03m# _wait_for_data() will resume reading if stream was paused.\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreaduntil\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isep \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limit:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/asyncio/streams.py:501\u001b[0m, in \u001b[0;36mStreamReader._wait_for_data\u001b[0;34m(self, func_name)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#register an account at ngrok.com and create an authtoken and place it here\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m     28\u001b[0m     run_process([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mngrok\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd-authtoken\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m26bDGGPyD3XWVtyzK3dfK401oIq_74Wi1m84A7rv9bP5YnihG\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m     32\u001b[0m     run_process([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserve\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     33\u001b[0m     run_process([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mngrok\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--log\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m11434\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--host-header\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost:11434\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     34\u001b[0m )\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!pip install aiohttp pyngrok\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "# Set LD_LIBRARY_PATH so the system NVIDIA library \n",
    "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
    "\n",
    "async def run_process(cmd):\n",
    "  print('>>> starting', *cmd)\n",
    "  p = await asyncio.subprocess.create_subprocess_exec(\n",
    "      *cmd,\n",
    "      stdout=asyncio.subprocess.PIPE,\n",
    "      stderr=asyncio.subprocess.PIPE,\n",
    "  )\n",
    "\n",
    "  async def pipe(lines):\n",
    "    async for line in lines:\n",
    "      print(line.strip().decode('utf-8'))\n",
    "\n",
    "  await asyncio.gather(\n",
    "      pipe(p.stdout),\n",
    "      pipe(p.stderr),\n",
    "  )\n",
    "\n",
    "#register an account at ngrok.com and create an authtoken and place it here\n",
    "await asyncio.gather(\n",
    "    run_process(['ngrok', 'config', 'add-authtoken','26bDGGPyD3XWVtyzK3dfK401oIq_74Wi1m84A7rv9bP5YnihG'])\n",
    ")\n",
    "\n",
    "await asyncio.gather(\n",
    "    run_process(['ollama', 'serve']),\n",
    "    run_process(['ngrok', 'http', '--log', 'stderr', '11434', '--host-header', 'localhost:11434'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.9.5)\n",
      "Requirement already satisfied: pyngrok in /usr/local/python/3.10.13/lib/python3.10/site-packages (7.1.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp) (4.0.3)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from pyngrok) (6.0.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp) (3.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install aiohttp pyngrok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Datasets/population.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>339996563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China</td>\n",
       "      <td>1425671352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>83294633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>85816199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japan</td>\n",
       "      <td>123294513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Country  Population\n",
       "0  United States   339996563\n",
       "1          China  1425671352\n",
       "2        Germany    83294633\n",
       "3         Turkey    85816199\n",
       "4          Japan   123294513"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama \n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasai import SmartDataframe \n",
    "\n",
    "df = SmartDataframe(data, config={\"llm\": llm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connection.py\", line 203, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 791, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 497, in _make_request\n",
      "    conn.request(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connection.py\", line 395, in request\n",
      "    self.endheaders()\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/http/client.py\", line 1278, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/http/client.py\", line 1038, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/http/client.py\", line 976, in send\n",
      "    self.connect()\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connection.py\", line 243, in connect\n",
      "    self.sock = self._new_conn()\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connection.py\", line 218, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x71a7471342b0>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 845, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/util/retry.py\", line 515, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71a7471342b0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pandasai/pipelines/chat/generate_chat_pipeline.py\", line 307, in run\n",
      "    output = (self.code_generation_pipeline | self.code_execution_pipeline).run(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pandasai/pipelines/pipeline.py\", line 137, in run\n",
      "    raise e\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pandasai/pipelines/pipeline.py\", line 101, in run\n",
      "    step_output = logic.execute(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pandasai/pipelines/chat/code_generator.py\", line 33, in execute\n",
      "    code = pipeline_context.config.llm.generate_code(input, pipeline_context)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pandasai/llm/base.py\", line 200, in generate_code\n",
      "    response = self.call(instruction, context)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pandasai/llm/langchain.py\", line 55, in call\n",
      "    res = self.langchain_llm.invoke(prompt)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 276, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 633, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 803, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 670, in _generate_helper\n",
      "    raise e\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 657, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_community/llms/ollama.py\", line 417, in _generate\n",
      "    final_chunk = super()._stream_with_aggregation(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_community/llms/ollama.py\", line 326, in _stream_with_aggregation\n",
      "    for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_community/llms/ollama.py\", line 172, in _create_generate_stream\n",
      "    yield from self._create_stream(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_community/llms/ollama.py\", line 231, in _create_stream\n",
      "    response = requests.post(\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71a7471342b0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, I was not able to answer your question, because of the following error:\\n\\nHTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71a7471342b0>: Failed to establish a new connection: [Errno 111] Connection refused'))\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.chat(\"Which are the top 5 countries by population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandasai import SmartDataframe\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"],\n",
    "    \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064],\n",
    "    \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12]\n",
    "})\n",
    "\n",
    "# Instantiate a LLM\n",
    "from pandasai.llm import OpenAI\n",
    "llm = OpenAI(api_token=\"YOUR_API_TOKEN\")\n",
    "\n",
    "df = SmartDataframe(df, config={\"llm\": llm})\n",
    "df.chat('Which are the 5 happiest countries?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connection.py\", line 203, in _new_conn\n",
      "    sock = connection.create_connection(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    raise err\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 791, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 497, in _make_request\n",
      "    conn.request(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connection.py\", line 395, in request\n",
      "    self.endheaders()\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/http/client.py\", line 1278, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/http/client.py\", line 1038, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/http/client.py\", line 976, in send\n",
      "    self.connect()\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connection.py\", line 243, in connect\n",
      "    self.sock = self._new_conn()\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connection.py\", line 218, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x71a747135570>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 845, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/urllib3/util/retry.py\", line 515, in increment\n",
      "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71a747135570>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pandasai/pipelines/chat/generate_chat_pipeline.py\", line 307, in run\n",
      "    output = (self.code_generation_pipeline | self.code_execution_pipeline).run(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pandasai/pipelines/pipeline.py\", line 137, in run\n",
      "    raise e\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pandasai/pipelines/pipeline.py\", line 101, in run\n",
      "    step_output = logic.execute(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pandasai/pipelines/chat/code_generator.py\", line 33, in execute\n",
      "    code = pipeline_context.config.llm.generate_code(input, pipeline_context)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pandasai/llm/base.py\", line 200, in generate_code\n",
      "    response = self.call(instruction, context)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/pandasai/llm/langchain.py\", line 55, in call\n",
      "    res = self.langchain_llm.invoke(prompt)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 276, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 633, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 803, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 670, in _generate_helper\n",
      "    raise e\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 657, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_community/llms/ollama.py\", line 417, in _generate\n",
      "    final_chunk = super()._stream_with_aggregation(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_community/llms/ollama.py\", line 326, in _stream_with_aggregation\n",
      "    for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_community/llms/ollama.py\", line 172, in _create_generate_stream\n",
      "    yield from self._create_stream(\n",
      "  File \"/home/codespace/.python/current/lib/python3.10/site-packages/langchain_community/llms/ollama.py\", line 231, in _create_stream\n",
      "    response = requests.post(\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/requests/api.py\", line 115, in post\n",
      "    return request(\"post\", url, data=data, json=json, **kwargs)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/requests/api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/codespace/.local/lib/python3.10/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71a747135570>: Failed to establish a new connection: [Errno 111] Connection refused'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, I was not able to answer your question, because of the following error:\\n\\nHTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71a747135570>: Failed to establish a new connection: [Errno 111] Connection refused'))\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.chat(\"What is the total populations of the top 5 countries by population?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
